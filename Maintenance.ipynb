{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c056764f-e4d3-4a88-9f9b-0b247cbf7805",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from variables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04f31c30-1da6-4efe-a61c-1c43f7dafbe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Delete all tables in your schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "44e33ee2-d8ba-45a1-92c7-d0ac29f8a21d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"--- Starting to drop tables ---\")\n",
    "for schema in [BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA]:\n",
    "  full_schema_name = f\"{CATALOG_NAME}.{schema}\"\n",
    "  try:\n",
    "    print(f\"\\nProcessing schema: {full_schema_name}\")\n",
    "    # Get all tables in the current catalog and schema\n",
    "    tables_df = spark.sql(f\"SHOW TABLES IN {full_schema_name}\")\n",
    "    \n",
    "    # Collect table names to avoid issues with modifying the list while iterating\n",
    "    tables_to_check = [row.tableName for row in tables_df.collect()]\n",
    "\n",
    "    for table_name in tables_to_check:\n",
    "      full_table_name = f\"{full_schema_name}.{table_name}\"\n",
    "      # Check if the table name does not start with 'bridge'\n",
    "      try:\n",
    "          #print(f\"  Dropping table: {full_table_name}\")\n",
    "          spark.sql(f\"DROP TABLE {full_table_name}\")\n",
    "          print(f\"  SUCCESS: Dropped {full_table_name}\")\n",
    "      except Exception as e:\n",
    "          print(f\"  FAILED to drop {full_table_name}: {e}\")\n",
    "\n",
    "  except Exception as e:\n",
    "    print(f\"Could not process schema '{full_schema_name}': {e}\")\n",
    "\n",
    "print(\"\\n--- Deletion process complete ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afa8729f-1a80-4957-945d-271f9ad8b40d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Remove all raw files from the data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfd961cb-5008-4cc5-81d6-0ca414224637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for path in [RAW_SALES_PATH, RAW_CUSTOMERS_PATH, RAW_PRODUCTS_PATH, RAW_STORES_PATH]:\n",
    "    dbutils.fs.rm(path, recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a07a84d4-8349-4446-891b-5d4fe92c7664",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### List parquet files that the data generator created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f373a7a5-3d11-4b14-9728-7deb7b36eec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for path in [RAW_SALES_PATH, RAW_CUSTOMERS_PATH, RAW_PRODUCTS_PATH, RAW_STORES_PATH]:\n",
    "    display(dbutils.fs.ls(path)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e54046fe-d5c4-4e03-9cb2-b2e9fe8a6a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### HEALTH CHECK\n",
    "First 4 rows display record counts for sales, customers, items, and stores raw data\n",
    "\n",
    "Next 3 rows checks referential integrity - if there are any customers, products, or stores in the fact table, which are not in the lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a544aa64-9880-4323-a25c-d43c0addff70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"\"\"\n",
    "SELECT \"sales_count\" as table_name, count(*) as record_count FROM delta.`{RAW_SALES_PATH}`\n",
    "UNION \n",
    "SELECT \"customers_count\", count(*) FROM delta.`{RAW_CUSTOMERS_PATH}`\n",
    "UNION\n",
    "SELECT \"items_count\", count(*) FROM delta.`{RAW_PRODUCTS_PATH}`\n",
    "UNION\n",
    "SELECT \"stores_count\", count(*) FROM delta.`{RAW_STORES_PATH}`\n",
    "UNION\n",
    "SELECT \"missing_customers\", COUNT(*)\n",
    "FROM delta.`{RAW_SALES_PATH}` s\n",
    "LEFT ANTI JOIN delta.`{RAW_CUSTOMERS_PATH}` c\n",
    "ON s.customer_id = c.customer_id\n",
    "UNION\n",
    "SELECT \"missing_products\", COUNT(*)\n",
    "FROM delta.`{RAW_SALES_PATH}` s\n",
    "LEFT ANTI JOIN delta.`{RAW_PRODUCTS_PATH}` i\n",
    "ON s.product_id = i.product_id\n",
    "UNION\n",
    "SELECT \"missing_stores\", COUNT(*)\n",
    "FROM delta.`{RAW_SALES_PATH}` s\n",
    "LEFT ANTI JOIN delta.`{RAW_STORES_PATH}` st\n",
    "ON s.store_id = st.store_id;\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Maintenance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
